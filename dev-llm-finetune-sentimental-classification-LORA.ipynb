{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0dfaf7-3bfd-487d-aea7-a3120ef09b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use LORA to fine tune a opt-350m model.\n",
    "#  GOAL:  + make the model train successful with LORA\n",
    "#         + record inference time, accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6145f20c-66d5-4923-8241-a05d8684b1af",
   "metadata": {},
   "source": [
    "dataset: use random 10% of imdb dataset, (seed=42)\n",
    "\n",
    "\n",
    "LORA:\n",
    "Recorded training:  506.0410854816437 seconds, 0.538 seconds per batch of batch size = 8\n",
    "memory footprint during training: 16376MiB\n",
    "accuracy: Evaluation accuracy: 0.8974\n",
    "\n",
    "Note: baseline LSTM RNN is .87 (https://www.kaggle.com/code/getanmolgupta01/imdb-sentiments-analysis-rnn-lstm-gru)\n",
    "\t        precision\trecall\tf1-score\tsupport\n",
    "0\t        0.894593\t0.848202\t0.870780\t5033.00000\n",
    "1\t        0.851506\t0.897011\t0.873666\t4884.00000\n",
    "accuracy\t0.872240\t0.872240\t0.872240\t0.87224\n",
    "macro avg\t0.873050\t0.872606\t0.872223\t9917.00000\n",
    "weighted avg\t0.873374\t0.872240\t0.872202\t9917.00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba082d6e-462e-4d8c-b4cd-4badd0264671",
   "metadata": {},
   "source": [
    "# Train sentimental classification model on imdb using facebook/opt-350m\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83dd9415-a8f8-4d43-9b9e-a4b0a727de65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "393af8c4-c449-47ea-850f-a865a26ce181",
   "metadata": {},
   "source": [
    "## Declare library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04494204-7ec9-4dbb-9672-a81933e9337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "\n",
    "MAX_LENGTH = 512 # length of the input \n",
    "DATA_PORTION = .1 # we use some % of the whole to imporve development speed. will set to 1.0 when the code is stable\n",
    "DATASET_SEED = 42# seed to randomize dataset\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "learning_rate = 2e-5\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1cf9a-d61d-4256-b491-3eecf63760fd",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "#### NOTE: to learn more about a structure of the dataset and dataloader, refer to `investigate_dataloader.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd49daef-8f3a-4b68-b88c-eecd22d4865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_dataset = dataset['train'].train_test_split(test_size=DATA_PORTION, shuffle=True, seed=DATASET_SEED)['test']\n",
    "test_dataset = dataset['test'].train_test_split(test_size=DATA_PORTION, shuffle=True, seed=DATASET_SEED)['test']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bdb37-8c75-40e2-ba07-3ba38838ae50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "668df527-3a58-4246-9267-89d9a4c2b792",
   "metadata": {},
   "source": [
    "## Load the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70be6207-1050-40c2-a189-c2cd68caeabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e25340-5db6-4893-8c18-b2bbc0579f0c",
   "metadata": {},
   "source": [
    "## Examine the tokenizer (more detail refer to `investigate_tokenizer.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cc1e6-ed31-4f9d-afee-16d36af7fb5d",
   "metadata": {},
   "source": [
    "## Use tokenizer to create tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c29b861-60ba-43ba-a3c1-45c3e129e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset and include labels\n",
    "def preprocess(examples):\n",
    "    result = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "    result['labels'] = examples['label']\n",
    "    return result\n",
    "\n",
    "if 'tokenized_train' not in globals():\n",
    "    tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "if 'tokenized_test' not in globals():\n",
    "    tokenized_test  = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Convert datasets to PyTorch tensors\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d76d19-3d94-43cd-aa45-85a41ba67b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this commented code if we want to delete tokenized train/test \n",
    "# del tokenized_train\n",
    "# del tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdad94-3c35-445e-8c1a-48b3b8ab63f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f59e78c6-7765-4f63-9684-7284f44b9f7d",
   "metadata": {},
   "source": [
    "#### Examine fields in tokenized sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc2b55e-4602-4fc3-b312-07df4f7ae8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']), 512, tensor(1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0].keys(), len(tokenized_train[0]['input_ids']), tokenized_train[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9121ba-fba5-4538-9aeb-51ba71921c86",
   "metadata": {},
   "source": [
    "#### Create train/test loader with batch_size from tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0bf7de-da46-4682-a282-9ba91f6e1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(tokenized_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(tokenized_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308218fb-dbca-40e3-8d79-de3723e2243e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78cb95b-be13-4bb8-850e-65443eedb96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24553a5c-604f-4481-83fa-2ceba7154ebf",
   "metadata": {},
   "source": [
    "## Full implementation of training iteration.\n",
    "##### We used detailed implementation for fine grain control of the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6ba64-5fb8-46c8-81a9-55e5e3a2e4f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fbc4b-1ba2-4653-98ed-de371a3fa967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf9fa5-1d12-491d-915a-4e5c5e47faac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e413902-c072-42fd-9f24-e7b5e20894ba",
   "metadata": {},
   "source": [
    "#### Configure LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596b24d9-bf1c-414d-9828-ca28dce6f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Task type for sequence classification\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=32,  # Alpha scaling parameter\n",
    "    lora_dropout=0.1  # Dropout rate\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497763e-c29d-46fa-a14b-e65297fe0617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921e3ed-9071-4337-9d60-5ec8ba6053fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27a931d-74d2-4b91-8ac8-272f8457ec82",
   "metadata": {},
   "source": [
    "#### Declare Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719bd9a0-ce14-436e-9040-bb5f49274d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Calculate number of training steps\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351845d0-de91-496c-89f4-a99525a7e410",
   "metadata": {},
   "source": [
    "#### Move model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2880376f-f753-41e7-8e7b-eda4e168de7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForSequenceClassification(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=512, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=512, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651e30c-711a-4c9d-9752-3829ccf3a202",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25f45b-b793-4b36-b1ad-a643fb17e1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b024333f-919c-427b-b54d-11392cbaa448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccec14a2c8a4debb27bc24901702be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 finished.\n",
      "Epoch 2/3 finished.\n",
      "Epoch 3/3 finished.\n",
      "Total training time: 506.0410854816437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "btime = time.time()\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} finished.\")\n",
    "    \n",
    "training_time = time.time() - btime\n",
    "print (f'Total training time: {training_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d46aa12-dc3b-40a5-9a39-51cc109039c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time per batch: 0.5389148940166599 seconds\n"
     ]
    }
   ],
   "source": [
    "print (f'Training time per batch: {training_time / num_training_steps} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac523d-2dc9-4a6b-bac2-946aff264e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eadc1b-95d7-4b4c-8b24-0270b3e4a5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c89c0a8b-97dd-4035-a498-70973eda4527",
   "metadata": {},
   "source": [
    "#### Evaluation loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5177c187-1df0-4dfe-810d-0cfca0015445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f8678b9e8b404d9af091273d133c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.8974\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "num_testing_steps = 1 * len(test_dataloader) \n",
    "progress_bar_eval = tqdm(range(num_testing_steps))\n",
    "\n",
    "model.eval()\n",
    "accuracy = 0\n",
    "num_eval_steps = 0\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    accuracy += (predictions == batch['labels']).float().mean().item()\n",
    "    num_eval_steps += 1\n",
    "\n",
    "    progress_bar_eval.update(1)\n",
    "\n",
    "accuracy = accuracy / num_eval_steps\n",
    "print(f\"Evaluation accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f164b3b-a9c4-45f2-83ac-5fdc6d551816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc9793-7708-452c-a79a-629d06367292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ef95d18-f865-4866-8953-26605e458fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 21 15:55:32 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40-24Q      On   | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   N/A    P8    N/A /  N/A |  16376MiB / 24370MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1026415      C   ...e/envs/env_llm/bin/python    14456MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6011a-2aba-44c3-b5eb-646a63d3693e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_llm)",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
